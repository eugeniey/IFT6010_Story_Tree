{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Implementation of TextRank\n(Based on: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)",
      "metadata": {
        "cell_id": "00000-c840c513-aa84-4f66-8bed-5091e8de1981",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "The input text is given below",
      "metadata": {
        "cell_id": "00001-d01223d6-6d2f-46ff-a92b-70263db5d93c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00002-94d0d4fd-0cd4-4afe-ba17-770d7db99ab8",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "df36b622",
        "execution_start": 1616527331308,
        "execution_millis": 0,
        "deepnote_cell_type": "code"
      },
      "source": "#Source of text:\n#https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents\n\nText = \"Compatibility of systems of linear constraints over the set of natural numbers. \\\nCriteria of compatibility of a system of linear Diophantine equations, strict inequations, and \\\nnonstrict inequations are considered. \\\nUpper bounds for components of a minimal set of solutions and \\\nalgorithms of construction of minimal generating sets of solutions for all \\\ntypes of systems are given. \\\nThese criteria and the corresponding algorithms for constructing \\\na minimal supporting set of solutions can be used in solving all the \\\nconsidered types of systems and systems of mixed types.\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Cleaning Text Data\n\nThe raw input text is cleaned off non-printable characters (if any) and turned into lower case.\nThe processed input text is then tokenized using NLTK library functions. ",
      "metadata": {
        "cell_id": "00003-bf0691e7-cd9e-40f6-ac25-38bbbf64b43a",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00004-5ced2455-46a6-4d94-9e8a-b5a9a2f50a43",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b63677da",
        "execution_start": 1616527333578,
        "execution_millis": 870,
        "deepnote_cell_type": "code"
      },
      "source": "\nimport nltk\nfrom nltk import word_tokenize\nimport string\n\n#nltk.download('punkt')\n\ndef clean(text):\n    text = text.lower()\n    printable = set(string.printable)\n    text = filter(lambda x: x in printable, text)\n    text = \"\".join(list(text))\n    return text\n\nCleaned_text = clean(Text)\n# print(Cleaned_text)\ntext = word_tokenize(Cleaned_text)\n\nprint (\"Tokenized Text: \\n\")\nprint (text)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Tokenized Text: \n\n['compatibility', 'of', 'systems', 'of', 'linear', 'constraints', 'over', 'the', 'set', 'of', 'natural', 'numbers', '.', 'criteria', 'of', 'compatibility', 'of', 'a', 'system', 'of', 'linear', 'diophantine', 'equations', ',', 'strict', 'inequations', ',', 'and', 'nonstrict', 'inequations', 'are', 'considered', '.', 'upper', 'bounds', 'for', 'components', 'of', 'a', 'minimal', 'set', 'of', 'solutions', 'and', 'algorithms', 'of', 'construction', 'of', 'minimal', 'generating', 'sets', 'of', 'solutions', 'for', 'all', 'types', 'of', 'systems', 'are', 'given', '.', 'these', 'criteria', 'and', 'the', 'corresponding', 'algorithms', 'for', 'constructing', 'a', 'minimal', 'supporting', 'set', 'of', 'solutions', 'can', 'be', 'used', 'in', 'solving', 'all', 'the', 'considered', 'types', 'of', 'systems', 'and', 'systems', 'of', 'mixed', 'types', '.']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### POS Tagging For Lemmatization\n\nNLTK is again used for <b>POS tagging</b> the input text so that the words can be lemmatized based on their POS tags.\n\nDescription of POS tags: \n\n\nhttp://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html",
      "metadata": {
        "cell_id": "00005-7e2f38a9-d976-4e23-9298-4971df8b8b29",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00006-8e1fb308-139b-4d9b-ae74-e4fe79324009",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "99917cd2",
        "execution_millis": 280,
        "execution_start": 1616527341144,
        "deepnote_cell_type": "code"
      },
      "source": "nltk.download('averaged_perceptron_tagger')\n  \nPOS_tag = nltk.pos_tag(text)\n\nprint (\"Tokenized Text with POS tags: \\n\")\nprint (POS_tag)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\nTokenized Text with POS tags: \n\n[('compatibility', 'NN'), ('of', 'IN'), ('systems', 'NNS'), ('of', 'IN'), ('linear', 'JJ'), ('constraints', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('set', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('numbers', 'NNS'), ('.', '.'), ('criteria', 'NNS'), ('of', 'IN'), ('compatibility', 'NN'), ('of', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('diophantine', 'NN'), ('equations', 'NNS'), (',', ','), ('strict', 'JJ'), ('inequations', 'NNS'), (',', ','), ('and', 'CC'), ('nonstrict', 'JJ'), ('inequations', 'NNS'), ('are', 'VBP'), ('considered', 'VBN'), ('.', '.'), ('upper', 'JJ'), ('bounds', 'NNS'), ('for', 'IN'), ('components', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('minimal', 'JJ'), ('set', 'NN'), ('of', 'IN'), ('solutions', 'NNS'), ('and', 'CC'), ('algorithms', 'NN'), ('of', 'IN'), ('construction', 'NN'), ('of', 'IN'), ('minimal', 'JJ'), ('generating', 'VBG'), ('sets', 'NNS'), ('of', 'IN'), ('solutions', 'NNS'), ('for', 'IN'), ('all', 'DT'), ('types', 'NNS'), ('of', 'IN'), ('systems', 'NNS'), ('are', 'VBP'), ('given', 'VBN'), ('.', '.'), ('these', 'DT'), ('criteria', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('corresponding', 'JJ'), ('algorithms', 'NN'), ('for', 'IN'), ('constructing', 'VBG'), ('a', 'DT'), ('minimal', 'JJ'), ('supporting', 'NN'), ('set', 'NN'), ('of', 'IN'), ('solutions', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('in', 'IN'), ('solving', 'VBG'), ('all', 'PDT'), ('the', 'DT'), ('considered', 'VBN'), ('types', 'NNS'), ('of', 'IN'), ('systems', 'NNS'), ('and', 'CC'), ('systems', 'NNS'), ('of', 'IN'), ('mixed', 'JJ'), ('types', 'NNS'), ('.', '.')]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Lemmatization\n\nThe tokenized text (mainly the nouns and adjectives) is normalized by <b>lemmatization</b>.\nIn lemmatization different grammatical counterparts of a word will be replaced by single\nbasic lemma. For example, 'glasses' may be replaced by 'glass'. \n\nDetails about lemmatization: \n    \nhttps://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html",
      "metadata": {
        "cell_id": "00007-4d28c4e9-93b9-46d9-bc0c-85bccf8cedaa",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00008-c28e0891-a88b-4ad3-9c08-3b7aa909dc70",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c7e096ae",
        "execution_start": 1616527347782,
        "execution_millis": 1842,
        "deepnote_cell_type": "code"
      },
      "source": "nltk.download('wordnet')\n\nfrom nltk.stem import WordNetLemmatizer\n\nwordnet_lemmatizer = WordNetLemmatizer()\n\nadjective_tags = ['JJ','JJR','JJS']\n\nlemmatized_text = []\n\nfor word in POS_tag:\n    if word[1] in adjective_tags:\n        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n    else:\n        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n        \nprint (\"Text tokens after lemmatization of adjectives and nouns: \\n\")\nprint (lemmatized_text)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\nText tokens after lemmatization of adjectives and nouns: \n\n['compatibility', 'of', 'system', 'of', 'linear', 'constraint', 'over', 'the', 'set', 'of', 'natural', 'number', '.', 'criterion', 'of', 'compatibility', 'of', 'a', 'system', 'of', 'linear', 'diophantine', 'equation', ',', 'strict', 'inequations', ',', 'and', 'nonstrict', 'inequations', 'are', 'considered', '.', 'upper', 'bound', 'for', 'component', 'of', 'a', 'minimal', 'set', 'of', 'solution', 'and', 'algorithm', 'of', 'construction', 'of', 'minimal', 'generating', 'set', 'of', 'solution', 'for', 'all', 'type', 'of', 'system', 'are', 'given', '.', 'these', 'criterion', 'and', 'the', 'corresponding', 'algorithm', 'for', 'constructing', 'a', 'minimal', 'supporting', 'set', 'of', 'solution', 'can', 'be', 'used', 'in', 'solving', 'all', 'the', 'considered', 'type', 'of', 'system', 'and', 'system', 'of', 'mixed', 'type', '.']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### POS tagging for Filtering\n\nThe <b>lemmatized text</b> is <b>POS tagged</b> here. The tags will be used for filtering later on.",
      "metadata": {
        "cell_id": "00009-2f5e003a-3c4b-473a-95d7-3605168e9cbd",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00010-05c18a4c-2794-4121-82e8-44142eb42cac",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "750aef43",
        "execution_start": 1616527352092,
        "execution_millis": 10,
        "deepnote_cell_type": "code"
      },
      "source": "POS_tag = nltk.pos_tag(lemmatized_text)\n\nprint (\"Lemmatized text with POS tags: \\n\")\nprint (POS_tag)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Lemmatized text with POS tags: \n\n[('compatibility', 'NN'), ('of', 'IN'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('constraint', 'NN'), ('over', 'IN'), ('the', 'DT'), ('set', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('number', 'NN'), ('.', '.'), ('criterion', 'NN'), ('of', 'IN'), ('compatibility', 'NN'), ('of', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('diophantine', 'JJ'), ('equation', 'NN'), (',', ','), ('strict', 'JJ'), ('inequations', 'NNS'), (',', ','), ('and', 'CC'), ('nonstrict', 'JJ'), ('inequations', 'NNS'), ('are', 'VBP'), ('considered', 'VBN'), ('.', '.'), ('upper', 'JJ'), ('bound', 'NN'), ('for', 'IN'), ('component', 'NN'), ('of', 'IN'), ('a', 'DT'), ('minimal', 'JJ'), ('set', 'NN'), ('of', 'IN'), ('solution', 'NN'), ('and', 'CC'), ('algorithm', 'NN'), ('of', 'IN'), ('construction', 'NN'), ('of', 'IN'), ('minimal', 'JJ'), ('generating', 'VBG'), ('set', 'NN'), ('of', 'IN'), ('solution', 'NN'), ('for', 'IN'), ('all', 'DT'), ('type', 'NN'), ('of', 'IN'), ('system', 'NN'), ('are', 'VBP'), ('given', 'VBN'), ('.', '.'), ('these', 'DT'), ('criterion', 'NN'), ('and', 'CC'), ('the', 'DT'), ('corresponding', 'JJ'), ('algorithm', 'NN'), ('for', 'IN'), ('constructing', 'VBG'), ('a', 'DT'), ('minimal', 'JJ'), ('supporting', 'NN'), ('set', 'NN'), ('of', 'IN'), ('solution', 'NN'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('in', 'IN'), ('solving', 'VBG'), ('all', 'PDT'), ('the', 'DT'), ('considered', 'VBN'), ('type', 'NN'), ('of', 'IN'), ('system', 'NN'), ('and', 'CC'), ('system', 'NN'), ('of', 'IN'), ('mixed', 'JJ'), ('type', 'NN'), ('.', '.')]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## POS Based Filtering\n\nAny word from the lemmatized text, which isn't a noun, adjective, or gerund (or a 'foreign word'), is here\nconsidered as a <b>stopword</b> (non-content). This is based on the assumption that usually keywords are noun,\nadjectives or gerunds. \n\nPunctuations are added to the stopword list too.",
      "metadata": {
        "cell_id": "00011-2b883014-32d7-4803-9465-e141db64ba92",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00012-72cc8e8c-3888-41b3-8751-3437753554fe",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "47d5fba3",
        "execution_start": 1616527354239,
        "execution_millis": 3,
        "deepnote_cell_type": "code"
      },
      "source": "stopwords = []\n\nwanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n\nfor word in POS_tag:\n    if word[1] not in wanted_POS:\n        stopwords.append(word[0])\n\npunctuations = list(str(string.punctuation))\n\nstopwords = stopwords + punctuations",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Complete stopword generation\n\nEven if we remove the aforementioned stopwords, still some extremely common nouns, adjectives or gerunds may\nremain which are very bad candidates for being keywords (or part of it). \n\nAn external file constituting a long list of stopwords is loaded and all the words are added with the previous\nstopwords to create the final list 'stopwords-plus' which is then converted into a set. \n\n(Source of stopwords data: https://www.ranks.nl/stopwords)\n\nStopwords-plus constitute the sum total of all stopwords and potential phrase-delimiters. \n\n(The contents of this set will be later used to partition the lemmatized text into n-gram phrases. But, for now, I will simply remove the stopwords, and work with a 'bag-of-words' approach. I will be developing the graph using unigram texts as vertices)",
      "metadata": {
        "cell_id": "00013-1f9111fb-a0ee-472a-b913-f3846b51686b",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00014-0edefa3e-eaa1-4997-a03b-7cca48478591",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "ad8f3ca4",
        "execution_start": 1616527392637,
        "execution_millis": 584,
        "deepnote_cell_type": "code"
      },
      "source": "!ls",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "archive\t\t\t  keywordGraph_CDlib.ipynb  TextRank.ipynb\r\ndocument_to_tf_idf.ipynb  keywordGraph.ipynb\t    xml_to_csv.ipynb\r\nkeyword_extractor.ipynb   random_stuff.ipynb\r\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00014-08479b20-ae59-400a-9044-5e67a6e7a791",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "1f04996c",
        "execution_millis": 5,
        "execution_start": 1616527429543,
        "deepnote_cell_type": "code"
      },
      "source": "stopword_file = open(\"long_stopwords.txt\", \"r\")\n#Source = https://www.ranks.nl/stopwords\n\nlots_of_stopwords = []\n\nfor line in stopword_file.readlines():\n    lots_of_stopwords.append(str(line.strip()))\n\nstopwords_plus = []\nstopwords_plus = stopwords + lots_of_stopwords\nstopwords_plus = set(stopwords_plus)\n\n#Stopwords_plus contain total set of all stopwords",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Removing Stopwords \n\nRemoving stopwords from lemmatized_text. \nProcesseced_text condtains the result.",
      "metadata": {
        "tags": [],
        "cell_id": "00016-42a2e38a-f007-4a2f-8ef1-1b8253a89ddf",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00016-63f20fac-d2e1-4bce-a4a9-7aaa2bf8b3d4",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "ca0b9440",
        "execution_start": 1616527446201,
        "execution_millis": 8,
        "deepnote_cell_type": "code"
      },
      "source": "processed_text = []\nfor word in lemmatized_text:\n    if word not in stopwords_plus:\n        processed_text.append(word)\nprint (processed_text)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['compatibility', 'system', 'linear', 'constraint', 'set', 'natural', 'number', 'criterion', 'compatibility', 'system', 'linear', 'diophantine', 'equation', 'strict', 'inequations', 'nonstrict', 'inequations', 'upper', 'bound', 'component', 'minimal', 'set', 'solution', 'algorithm', 'construction', 'minimal', 'generating', 'set', 'solution', 'type', 'system', 'criterion', 'algorithm', 'constructing', 'minimal', 'supporting', 'set', 'solution', 'solving', 'type', 'system', 'system', 'mixed', 'type']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Vocabulary Creation\n\nVocabulary will only contain unique words from processed_text.",
      "metadata": {
        "cell_id": "00017-0fc041b0-3a19-4afe-a7db-9ee9b9e2ede6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00018-33b30d0f-d75a-488a-95e4-898f0bf3bd0c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d3adb10f",
        "execution_start": 1616527448379,
        "execution_millis": 8,
        "deepnote_cell_type": "code"
      },
      "source": "vocabulary = list(set(processed_text))\nprint (vocabulary)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['solving', 'supporting', 'system', 'compatibility', 'solution', 'upper', 'bound', 'minimal', 'set', 'linear', 'mixed', 'type', 'inequations', 'constructing', 'equation', 'algorithm', 'natural', 'generating', 'number', 'diophantine', 'criterion', 'nonstrict', 'construction', 'strict', 'constraint', 'component']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Building Graph\n\nTextRank is a graph based model, and thus it requires us to build a graph. Each words in the vocabulary will serve as a vertex for graph. The words will be represented in the vertices by their index in vocabulary list.  \n\nThe weighted_edge matrix contains the information of edge connections among all vertices.\nI am building wieghted undirected edges.\n\nweighted_edge[i][j] contains the weight of the connecting edge between the word vertex represented by vocabulary index i and the word vertex represented by vocabulary j.\n\nIf weighted_edge[i][j] is zero, it means no edge connection is present between the words represented by index i and j.\n\nThere is a connection between the words (and thus between i and j which represents them) if the words co-occur within a window of a specified 'window_size' in the processed_text.\n\nThe value of the weighted_edge[i][j] is increased by (1/(distance between positions of words currently represented by i and j)) for every connection discovered between the same words in different locations of the text. \n\nThe covered_coocurrences list (which is contain the list of pairs of absolute positions in processed_text of the words whose coocurrence at that location is already checked) is managed so that the same two words located in the same positions in processed_text are not repetitively counted while sliding the window one text unit at a time.\n\nThe score of all vertices are intialized to one. \n\nSelf-connections are not considered, so weighted_edge[i][i] will be zero.",
      "metadata": {
        "cell_id": "00019-0d5468b8-4986-4cbe-baa8-ebcbc543a26f",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00021-4f0d41cc-9b28-48ed-b021-18a9d6c174ed",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "2962e527",
        "execution_start": 1616527597175,
        "execution_millis": 4,
        "deepnote_cell_type": "code"
      },
      "source": "len(processed_text)",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "44"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00020-f55809f4-9d5d-456f-a8ec-7d6d2ef7a12f",
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\nimport math\nvocab_len = len(vocabulary)\n\nweighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n\nscore = np.zeros((vocab_len),dtype=np.float32)\nwindow_size = 3\ncovered_coocurrences = []\n\nfor i in range(0,vocab_len):\n    score[i]=1\n    for j in range(0,vocab_len):\n        if j==i:\n            weighted_edge[i][j]=0\n        else:\n            for window_start in range(0,(len(processed_text)-window_size)):\n                \n                window_end = window_start+window_size\n                \n                window = processed_text[window_start:window_end]\n                \n                if (vocabulary[i] in window) and (vocabulary[j] in window):\n                    \n                    index_of_i = window_start + window.index(vocabulary[i])\n                    index_of_j = window_start + window.index(vocabulary[j])\n                    \n                    # index_of_x is the absolute position of the xth term in the window \n                    # (counting from 0) \n                    # in the processed_text\n                      \n                    if [index_of_i,index_of_j] not in covered_coocurrences:\n                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n                        covered_coocurrences.append([index_of_i,index_of_j])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Calculating weighted summation of connections of a vertex\n\ninout[i] will contain the sum of all the undirected connections\\edges associated withe the vertex represented by i.",
      "metadata": {
        "cell_id": "00021-cb8bcfdf-5e8a-417f-a659-4262000ee1f5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00022-1088a28c-bfe9-4680-b07a-72b4be0efb2b",
        "deepnote_cell_type": "code"
      },
      "source": "inout = np.zeros((vocab_len),dtype=np.float32)\n\nfor i in range(0,vocab_len):\n    for j in range(0,vocab_len):\n        inout[i]+=weighted_edge[i][j]",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Scoring Vertices\n\nThe formula used for scoring a vertex represented by i is:\n\nscore[i] = (1-d) + d x [ Summation(j) ( (weighted_edge[i][j]/inout[j]) x score[j] ) ] where j belongs to the list of vertieces that has a connection with i. \n\nd is the damping factor.\n\nThe score is iteratively updated until convergence. ",
      "metadata": {
        "cell_id": "00023-3cad5ab6-22f5-4729-96ae-26d4d71e6e51",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00024-5e57d205-a4d6-41d2-9c14-334282dbe6e1",
        "deepnote_cell_type": "code"
      },
      "source": "MAX_ITERATIONS = 50\nd=0.85\nthreshold = 0.0001 #convergence threshold\n\nfor iter in range(0,MAX_ITERATIONS):\n    prev_score = np.copy(score)\n    \n    for i in range(0,vocab_len):\n        \n        summation = 0\n        for j in range(0,vocab_len):\n            if weighted_edge[i][j] != 0:\n                summation += (weighted_edge[i][j]/inout[j])*score[j]\n                \n        score[i] = (1-d) + d*(summation)\n    \n    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n        print(\"Converging at iteration \"+str(iter)+\"....\")\n        break\n",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Converging at iteration 23....\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00025-49a0a935-9bb7-4e20-aaf0-533d05f4a407",
        "deepnote_cell_type": "code"
      },
      "source": "for i in range(0,vocab_len):\n    print(\"Score of \"+vocabulary[i]+\": \"+str(score[i]))",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Score of solving: 0.64231944\nScore of equation: 0.79981786\nScore of generating: 0.65264744\nScore of diophantine: 0.759297\nScore of construction: 0.6598107\nScore of set: 2.2718465\nScore of mixed: 0.2358227\nScore of minimal: 1.7869267\nScore of compatibility: 0.9445859\nScore of component: 0.73764145\nScore of system: 2.1203177\nScore of natural: 0.6883006\nScore of inequations: 1.308244\nScore of constraint: 0.67441183\nScore of criterion: 1.2255884\nScore of type: 1.0810083\nScore of upper: 0.8167923\nScore of solution: 1.683202\nScore of linear: 1.2716976\nScore of algorithm: 1.1936545\nScore of strict: 0.8237729\nScore of bound: 0.78600633\nScore of nonstrict: 0.8272164\nScore of number: 0.6883157\nScore of supporting: 0.6537049\nScore of constructing: 0.66728705\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Phrase Partiotioning\n\nParitioning lemmatized_text into phrases using the stopwords in it as delimeters.\nThe phrases are also candidates for keyphrases to be extracted. ",
      "metadata": {
        "cell_id": "00026-a043fb9f-1d74-4822-82b6-944699e0dfb2",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00027-f5c22dca-0e14-46de-9b39-8e9ea3fcc543",
        "deepnote_cell_type": "code"
      },
      "source": "phrases = []\n\nphrase = \" \"\nfor word in lemmatized_text:\n    \n    if word in stopwords_plus:\n        if phrase!= \" \":\n            phrases.append(str(phrase).strip().split())\n        phrase = \" \"\n    elif word not in stopwords_plus:\n        phrase+=str(word)\n        phrase+=\" \"\n\nprint(\"Partitioned Phrases (Candidate Keyphrases): \\n\")\nprint(phrases)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Partitioned Phrases (Candidate Keyphrases): \n\n[['compatibility'], ['system'], ['linear', 'constraint'], ['set'], ['natural', 'number'], ['criterion'], ['compatibility'], ['system'], ['linear', 'diophantine', 'equation'], ['strict', 'inequations'], ['nonstrict', 'inequations'], ['upper', 'bound'], ['component'], ['minimal', 'set'], ['solution'], ['algorithm'], ['construction'], ['minimal', 'generating', 'set'], ['solution'], ['type'], ['system'], ['criterion'], ['algorithm'], ['constructing'], ['minimal', 'supporting', 'set'], ['solution'], ['solving'], ['type'], ['system'], ['system'], ['mixed', 'type']]\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Create a list of unique phrases.\n\nRepeating phrases\\keyphrase candidates has no purpose here, anymore. ",
      "metadata": {
        "cell_id": "00028-e09f8eb1-b3ca-41cc-993c-ee80e2d60e4f",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00029-27f0d949-a839-4b8e-a122-fd72d7998c5d",
        "deepnote_cell_type": "code"
      },
      "source": "unique_phrases = []\n\nfor phrase in phrases:\n    if phrase not in unique_phrases:\n        unique_phrases.append(phrase)\n\nprint(\"Unique Phrases (Candidate Keyphrases): \\n\")\nprint(unique_phrases)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Unique Phrases (Candidate Keyphrases): \n\n[['compatibility'], ['system'], ['linear', 'constraint'], ['set'], ['natural', 'number'], ['criterion'], ['linear', 'diophantine', 'equation'], ['strict', 'inequations'], ['nonstrict', 'inequations'], ['upper', 'bound'], ['component'], ['minimal', 'set'], ['solution'], ['algorithm'], ['construction'], ['minimal', 'generating', 'set'], ['type'], ['constructing'], ['minimal', 'supporting', 'set'], ['solving'], ['mixed', 'type']]\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Thinning the list of candidate-keyphrases.\n\nRemoving single word keyphrases-candidates that are present multi-word alternatives. ",
      "metadata": {
        "cell_id": "00030-c075517e-e8b4-4b5f-8f94-447dceed721b",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00031-19710c9d-770b-4b37-9ffe-7c24ed0ed753",
        "deepnote_cell_type": "code"
      },
      "source": "for word in vocabulary:\n    #print word\n    for phrase in unique_phrases:\n        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n            #if len(phrase)>1 then the current phrase is multi-worded.\n            #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n            # and at the same time present as a word within a multi-worded phrase,\n            # then I will remove the single-word-phrase from the list.\n            unique_phrases.remove([word])\n            \nprint(\"Thinned Unique Phrases (Candidate Keyphrases): \\n\")\nprint(unique_phrases)    ",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Thinned Unique Phrases (Candidate Keyphrases): \n\n[['compatibility'], ['system'], ['linear', 'constraint'], ['natural', 'number'], ['criterion'], ['linear', 'diophantine', 'equation'], ['strict', 'inequations'], ['nonstrict', 'inequations'], ['upper', 'bound'], ['component'], ['minimal', 'set'], ['solution'], ['algorithm'], ['construction'], ['minimal', 'generating', 'set'], ['constructing'], ['minimal', 'supporting', 'set'], ['solving'], ['mixed', 'type']]\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Scoring Keyphrases\n\nScoring the phrases (candidate keyphrases) and building up a list of keyphrases\\keywords\nby listing untokenized versions of tokenized phrases\\candidate-keyphrases.\nPhrases are scored by adding the score of their members (words\\text-units that were ranked by the graph algorithm)\n",
      "metadata": {
        "cell_id": "00032-1a3f9ac6-6cfe-40ba-9593-811f2aaf04d8",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00033-13b61bf4-e980-44db-a19f-0989e1896423",
        "deepnote_cell_type": "code"
      },
      "source": "phrase_scores = []\nkeywords = []\nfor phrase in unique_phrases:\n    phrase_score=0\n    keyword = ''\n    for word in phrase:\n        keyword += str(word)\n        keyword += \" \"\n        phrase_score+=score[vocabulary.index(word)]\n    phrase_scores.append(phrase_score)\n    keywords.append(keyword.strip())\n\ni=0\nfor keyword in keywords:\n    print (\"Keyword: '\"+str(keyword)+\"', Score: \"+str(phrase_scores[i]))\n    i+=1",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Keyword: 'compatibility', Score: 0.944585919380188\nKeyword: 'system', Score: 2.1203176975250244\nKeyword: 'linear constraint', Score: 1.9461094737052917\nKeyword: 'natural number', Score: 1.3766162991523743\nKeyword: 'criterion', Score: 1.2255884408950806\nKeyword: 'linear diophantine equation', Score: 2.8308125138282776\nKeyword: 'strict inequations', Score: 2.132016897201538\nKeyword: 'nonstrict inequations', Score: 2.135460376739502\nKeyword: 'upper bound', Score: 1.6027986407279968\nKeyword: 'component', Score: 0.737641453742981\nKeyword: 'minimal set', Score: 4.0587732791900635\nKeyword: 'solution', Score: 1.6832020282745361\nKeyword: 'algorithm', Score: 1.1936545372009277\nKeyword: 'construction', Score: 0.6598107218742371\nKeyword: 'minimal generating set', Score: 4.711420714855194\nKeyword: 'constructing', Score: 0.6672870516777039\nKeyword: 'minimal supporting set', Score: 4.712478160858154\nKeyword: 'solving', Score: 0.6423194408416748\nKeyword: 'mixed type', Score: 1.3168310225009918\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Ranking Keyphrases\n\nRanking keyphrases based on their calculated scores. Displaying top keywords_num no. of keyphrases.",
      "metadata": {
        "cell_id": "00034-167ac5cf-043e-4344-bc38-7138e7386061",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00035-6dbb9624-417c-4833-b28a-7c06bc402478",
        "deepnote_cell_type": "code"
      },
      "source": "sorted_index = np.flip(np.argsort(phrase_scores),0)\n\nkeywords_num = 10\n\nprint(\"Keywords:\\n\")\n\nfor i in range(0,keywords_num):\n    print(str(keywords[sorted_index[i]])+\", \", end=' ')",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Keywords:\n\nminimal supporting set,  minimal generating set,  minimal set,  linear diophantine equation,  nonstrict inequations,  strict inequations,  system,  linear constraint,  solution,  upper bound,  "
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Input:\n\nCompatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types.\n\n# Extracted Keywords:\n\n* minimal supporting set,  \n* minimal generating set,  \n* minimal set,  \n* linear diophantine equation,  \n* nonstrict inequations,  \n* strict inequations,  \n* system,  \n* linear constraint,  \n* solution,  \n* upper bound, \n",
      "metadata": {
        "cell_id": "00036-dbdf7299-9642-43ee-9a05-d72b4e3122d0",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00037-879c28dc-2311-4f27-a4bb-eef2bd3670ab",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00038-a29c1d7f-74a3-4e91-88f8-d9a4cd9bc59b",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=902703f2-430f-48f3-ba3f-6c2fee66cf11' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "deepnote_notebook_id": "705cb9ef-2ee7-4033-af00-e838ae95f123",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}